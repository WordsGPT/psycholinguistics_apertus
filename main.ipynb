{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae90c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento a ejecutar\n",
    "\n",
    "experiment_name=\"EXPERIMENT_NAME\"           # A completar\n",
    "experiment_path=\"EXPERIMENT_PATH\"           # A completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización opcional de la sesión en Hugging Face (seguro)\n",
    "import os\n",
    "\n",
    "# Carga variables de entorno desde apis.env (si existe)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"api.env\")\n",
    "\n",
    "# Leer token y realizar login sólo si el token está presente\n",
    "api_key = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if api_key:\n",
    "    try:\n",
    "        from huggingface_hub import login\n",
    "        login(token=api_key)\n",
    "        print(\"Logged in to Hugging Face Hub\")\n",
    "    except Exception as e:\n",
    "        print(\"Hugging Face login failed:\", e)\n",
    "else:\n",
    "    print(\"No HUGGINGFACE_TOKEN found in apis.env — skipping Hugging Face login (public model access only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2547b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"swiss-ai/Apertus-8B-Instruct-2509\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Modelo cargado en dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar experimento\n",
    "\n",
    "from scripts.prepare_experiment import prepare_batches_for_experiment\n",
    "\n",
    "prepare_batches_for_experiment(experiment_name, experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ebc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar experimento\n",
    "\n",
    "from scripts.execute_experiment import run_experiment\n",
    "\n",
    "# Ejecutar el experimento (procesa solo palabras pendientes)\n",
    "df_results = run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    experiment_path=experiment_path,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    do_sample=False \n",
    ")\n",
    "\n",
    "print(f\"\\nTotal de palabras procesadas: {len(df_results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
